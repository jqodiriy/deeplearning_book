{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_size = 784\n",
    "hidden_size = 128\n",
    "output_size = 10\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = list(train.columns)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.tensor(train['label'].tolist()) \n",
    "X = torch.tensor(train[columns].values.tolist()) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(X)\n",
    "train_size = int(dataset_size * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "Y_train, Y_test = Y[:train_size], Y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/4200], Loss: 1.0223\n",
      "Epoch [1/10], Step [200/4200], Loss: 0.3495\n",
      "Epoch [1/10], Step [300/4200], Loss: 0.1064\n",
      "Epoch [1/10], Step [400/4200], Loss: 0.2063\n",
      "Epoch [1/10], Step [500/4200], Loss: 0.8299\n",
      "Epoch [1/10], Step [600/4200], Loss: 0.1185\n",
      "Epoch [1/10], Step [700/4200], Loss: 0.0445\n",
      "Epoch [1/10], Step [800/4200], Loss: 0.2905\n",
      "Epoch [1/10], Step [900/4200], Loss: 0.4036\n",
      "Epoch [1/10], Step [1000/4200], Loss: 0.1867\n",
      "Epoch [1/10], Step [1100/4200], Loss: 0.3817\n",
      "Epoch [1/10], Step [1200/4200], Loss: 0.0584\n",
      "Epoch [1/10], Step [1300/4200], Loss: 0.9072\n",
      "Epoch [1/10], Step [1400/4200], Loss: 0.2160\n",
      "Epoch [1/10], Step [1500/4200], Loss: 0.0162\n",
      "Epoch [1/10], Step [1600/4200], Loss: 0.0234\n",
      "Epoch [1/10], Step [1700/4200], Loss: 0.1656\n",
      "Epoch [1/10], Step [1800/4200], Loss: 0.3875\n",
      "Epoch [1/10], Step [1900/4200], Loss: 0.1220\n",
      "Epoch [1/10], Step [2000/4200], Loss: 0.1346\n",
      "Epoch [1/10], Step [2100/4200], Loss: 0.0458\n",
      "Epoch [1/10], Step [2200/4200], Loss: 0.0849\n",
      "Epoch [1/10], Step [2300/4200], Loss: 0.1201\n",
      "Epoch [1/10], Step [2400/4200], Loss: 0.3934\n",
      "Epoch [1/10], Step [2500/4200], Loss: 0.6734\n",
      "Epoch [1/10], Step [2600/4200], Loss: 0.3900\n",
      "Epoch [1/10], Step [2700/4200], Loss: 0.0151\n",
      "Epoch [1/10], Step [2800/4200], Loss: 0.0275\n",
      "Epoch [1/10], Step [2900/4200], Loss: 0.1029\n",
      "Epoch [1/10], Step [3000/4200], Loss: 0.4934\n",
      "Epoch [1/10], Step [3100/4200], Loss: 0.0623\n",
      "Epoch [1/10], Step [3200/4200], Loss: 0.1493\n",
      "Epoch [1/10], Step [3300/4200], Loss: 0.0447\n",
      "Epoch [1/10], Step [3400/4200], Loss: 0.3549\n",
      "Epoch [1/10], Step [3500/4200], Loss: 0.0456\n",
      "Epoch [1/10], Step [3600/4200], Loss: 0.4406\n",
      "Epoch [1/10], Step [3700/4200], Loss: 0.1421\n",
      "Epoch [1/10], Step [3800/4200], Loss: 0.0572\n",
      "Epoch [1/10], Step [3900/4200], Loss: 0.4539\n",
      "Epoch [1/10], Step [4000/4200], Loss: 0.2124\n",
      "Epoch [1/10], Step [4100/4200], Loss: 0.0259\n",
      "Epoch [1/10], Step [4200/4200], Loss: 0.1887\n",
      "Epoch [2/10], Step [100/4200], Loss: 0.0698\n",
      "Epoch [2/10], Step [200/4200], Loss: 1.0623\n",
      "Epoch [2/10], Step [300/4200], Loss: 0.0117\n",
      "Epoch [2/10], Step [400/4200], Loss: 0.1152\n",
      "Epoch [2/10], Step [500/4200], Loss: 0.0519\n",
      "Epoch [2/10], Step [600/4200], Loss: 0.0117\n",
      "Epoch [2/10], Step [700/4200], Loss: 0.0622\n",
      "Epoch [2/10], Step [800/4200], Loss: 0.0515\n",
      "Epoch [2/10], Step [900/4200], Loss: 0.0320\n",
      "Epoch [2/10], Step [1000/4200], Loss: 0.1670\n",
      "Epoch [2/10], Step [1100/4200], Loss: 0.0429\n",
      "Epoch [2/10], Step [1200/4200], Loss: 0.3247\n",
      "Epoch [2/10], Step [1300/4200], Loss: 0.3340\n",
      "Epoch [2/10], Step [1400/4200], Loss: 0.0045\n",
      "Epoch [2/10], Step [1500/4200], Loss: 0.1773\n",
      "Epoch [2/10], Step [1600/4200], Loss: 0.0050\n",
      "Epoch [2/10], Step [1700/4200], Loss: 0.0114\n",
      "Epoch [2/10], Step [1800/4200], Loss: 0.0879\n",
      "Epoch [2/10], Step [1900/4200], Loss: 0.1847\n",
      "Epoch [2/10], Step [2000/4200], Loss: 0.0895\n",
      "Epoch [2/10], Step [2100/4200], Loss: 0.3254\n",
      "Epoch [2/10], Step [2200/4200], Loss: 0.6367\n",
      "Epoch [2/10], Step [2300/4200], Loss: 0.1770\n",
      "Epoch [2/10], Step [2400/4200], Loss: 0.0311\n",
      "Epoch [2/10], Step [2500/4200], Loss: 0.1053\n",
      "Epoch [2/10], Step [2600/4200], Loss: 0.0804\n",
      "Epoch [2/10], Step [2700/4200], Loss: 0.9103\n",
      "Epoch [2/10], Step [2800/4200], Loss: 0.0121\n",
      "Epoch [2/10], Step [2900/4200], Loss: 0.0619\n",
      "Epoch [2/10], Step [3000/4200], Loss: 0.1617\n",
      "Epoch [2/10], Step [3100/4200], Loss: 0.4171\n",
      "Epoch [2/10], Step [3200/4200], Loss: 0.0238\n",
      "Epoch [2/10], Step [3300/4200], Loss: 0.0176\n",
      "Epoch [2/10], Step [3400/4200], Loss: 0.0190\n",
      "Epoch [2/10], Step [3500/4200], Loss: 0.2752\n",
      "Epoch [2/10], Step [3600/4200], Loss: 0.0309\n",
      "Epoch [2/10], Step [3700/4200], Loss: 0.0153\n",
      "Epoch [2/10], Step [3800/4200], Loss: 0.0308\n",
      "Epoch [2/10], Step [3900/4200], Loss: 0.1513\n",
      "Epoch [2/10], Step [4000/4200], Loss: 0.0095\n",
      "Epoch [2/10], Step [4100/4200], Loss: 0.0370\n",
      "Epoch [2/10], Step [4200/4200], Loss: 0.0236\n",
      "Epoch [3/10], Step [100/4200], Loss: 0.0046\n",
      "Epoch [3/10], Step [200/4200], Loss: 0.0255\n",
      "Epoch [3/10], Step [300/4200], Loss: 0.2098\n",
      "Epoch [3/10], Step [400/4200], Loss: 0.1224\n",
      "Epoch [3/10], Step [500/4200], Loss: 0.1458\n",
      "Epoch [3/10], Step [600/4200], Loss: 0.2654\n",
      "Epoch [3/10], Step [700/4200], Loss: 0.0023\n",
      "Epoch [3/10], Step [800/4200], Loss: 0.0047\n",
      "Epoch [3/10], Step [900/4200], Loss: 0.0010\n",
      "Epoch [3/10], Step [1000/4200], Loss: 0.0046\n",
      "Epoch [3/10], Step [1100/4200], Loss: 0.0095\n",
      "Epoch [3/10], Step [1200/4200], Loss: 0.0226\n",
      "Epoch [3/10], Step [1300/4200], Loss: 0.0008\n",
      "Epoch [3/10], Step [1400/4200], Loss: 0.0174\n",
      "Epoch [3/10], Step [1500/4200], Loss: 0.0060\n",
      "Epoch [3/10], Step [1600/4200], Loss: 0.0055\n",
      "Epoch [3/10], Step [1700/4200], Loss: 0.2638\n",
      "Epoch [3/10], Step [1800/4200], Loss: 0.1458\n",
      "Epoch [3/10], Step [1900/4200], Loss: 0.3761\n",
      "Epoch [3/10], Step [2000/4200], Loss: 0.4323\n",
      "Epoch [3/10], Step [2100/4200], Loss: 0.0056\n",
      "Epoch [3/10], Step [2200/4200], Loss: 0.0367\n",
      "Epoch [3/10], Step [2300/4200], Loss: 0.0041\n",
      "Epoch [3/10], Step [2400/4200], Loss: 0.0619\n",
      "Epoch [3/10], Step [2500/4200], Loss: 0.0335\n",
      "Epoch [3/10], Step [2600/4200], Loss: 0.0049\n",
      "Epoch [3/10], Step [2700/4200], Loss: 0.0041\n",
      "Epoch [3/10], Step [2800/4200], Loss: 0.0082\n",
      "Epoch [3/10], Step [2900/4200], Loss: 0.3969\n",
      "Epoch [3/10], Step [3000/4200], Loss: 0.0144\n",
      "Epoch [3/10], Step [3100/4200], Loss: 0.0018\n",
      "Epoch [3/10], Step [3200/4200], Loss: 0.0002\n",
      "Epoch [3/10], Step [3300/4200], Loss: 0.0650\n",
      "Epoch [3/10], Step [3400/4200], Loss: 0.0784\n",
      "Epoch [3/10], Step [3500/4200], Loss: 0.0073\n",
      "Epoch [3/10], Step [3600/4200], Loss: 0.0018\n",
      "Epoch [3/10], Step [3700/4200], Loss: 0.0026\n",
      "Epoch [3/10], Step [3800/4200], Loss: 0.3425\n",
      "Epoch [3/10], Step [3900/4200], Loss: 0.0041\n",
      "Epoch [3/10], Step [4000/4200], Loss: 0.0030\n",
      "Epoch [3/10], Step [4100/4200], Loss: 0.0002\n",
      "Epoch [3/10], Step [4200/4200], Loss: 0.0093\n",
      "Epoch [4/10], Step [100/4200], Loss: 0.0002\n",
      "Epoch [4/10], Step [200/4200], Loss: 0.0924\n",
      "Epoch [4/10], Step [300/4200], Loss: 0.0014\n",
      "Epoch [4/10], Step [400/4200], Loss: 0.0605\n",
      "Epoch [4/10], Step [500/4200], Loss: 0.0101\n",
      "Epoch [4/10], Step [600/4200], Loss: 0.0067\n",
      "Epoch [4/10], Step [700/4200], Loss: 0.0062\n",
      "Epoch [4/10], Step [800/4200], Loss: 0.0521\n",
      "Epoch [4/10], Step [900/4200], Loss: 0.0331\n",
      "Epoch [4/10], Step [1000/4200], Loss: 0.0007\n",
      "Epoch [4/10], Step [1100/4200], Loss: 0.0123\n",
      "Epoch [4/10], Step [1200/4200], Loss: 0.0112\n",
      "Epoch [4/10], Step [1300/4200], Loss: 0.0024\n",
      "Epoch [4/10], Step [1400/4200], Loss: 0.3306\n",
      "Epoch [4/10], Step [1500/4200], Loss: 0.0035\n",
      "Epoch [4/10], Step [1600/4200], Loss: 0.0094\n",
      "Epoch [4/10], Step [1700/4200], Loss: 0.0004\n",
      "Epoch [4/10], Step [1800/4200], Loss: 0.1392\n",
      "Epoch [4/10], Step [1900/4200], Loss: 0.0025\n",
      "Epoch [4/10], Step [2000/4200], Loss: 0.0007\n",
      "Epoch [4/10], Step [2100/4200], Loss: 0.0308\n",
      "Epoch [4/10], Step [2200/4200], Loss: 0.0160\n",
      "Epoch [4/10], Step [2300/4200], Loss: 0.0091\n",
      "Epoch [4/10], Step [2400/4200], Loss: 0.0307\n",
      "Epoch [4/10], Step [2500/4200], Loss: 0.0098\n",
      "Epoch [4/10], Step [2600/4200], Loss: 0.0016\n",
      "Epoch [4/10], Step [2700/4200], Loss: 0.0005\n",
      "Epoch [4/10], Step [2800/4200], Loss: 0.0037\n",
      "Epoch [4/10], Step [2900/4200], Loss: 0.0083\n",
      "Epoch [4/10], Step [3000/4200], Loss: 0.0029\n",
      "Epoch [4/10], Step [3100/4200], Loss: 0.0067\n",
      "Epoch [4/10], Step [3200/4200], Loss: 0.0412\n",
      "Epoch [4/10], Step [3300/4200], Loss: 0.0070\n",
      "Epoch [4/10], Step [3400/4200], Loss: 0.0016\n",
      "Epoch [4/10], Step [3500/4200], Loss: 0.0257\n",
      "Epoch [4/10], Step [3600/4200], Loss: 0.0012\n",
      "Epoch [4/10], Step [3700/4200], Loss: 0.0024\n",
      "Epoch [4/10], Step [3800/4200], Loss: 0.0014\n",
      "Epoch [4/10], Step [3900/4200], Loss: 0.0371\n",
      "Epoch [4/10], Step [4000/4200], Loss: 0.0051\n",
      "Epoch [4/10], Step [4100/4200], Loss: 0.0096\n",
      "Epoch [4/10], Step [4200/4200], Loss: 0.0006\n",
      "Epoch [5/10], Step [100/4200], Loss: 0.0015\n",
      "Epoch [5/10], Step [200/4200], Loss: 0.0008\n",
      "Epoch [5/10], Step [300/4200], Loss: 0.0389\n",
      "Epoch [5/10], Step [400/4200], Loss: 0.0000\n",
      "Epoch [5/10], Step [500/4200], Loss: 0.0014\n",
      "Epoch [5/10], Step [600/4200], Loss: 0.0009\n",
      "Epoch [5/10], Step [700/4200], Loss: 0.0002\n",
      "Epoch [5/10], Step [800/4200], Loss: 0.0665\n",
      "Epoch [5/10], Step [900/4200], Loss: 0.0372\n",
      "Epoch [5/10], Step [1000/4200], Loss: 0.0029\n",
      "Epoch [5/10], Step [1100/4200], Loss: 0.2714\n",
      "Epoch [5/10], Step [1200/4200], Loss: 0.0668\n",
      "Epoch [5/10], Step [1300/4200], Loss: 0.0004\n",
      "Epoch [5/10], Step [1400/4200], Loss: 0.0090\n",
      "Epoch [5/10], Step [1500/4200], Loss: 0.0121\n",
      "Epoch [5/10], Step [1600/4200], Loss: 0.0031\n",
      "Epoch [5/10], Step [1700/4200], Loss: 0.0485\n",
      "Epoch [5/10], Step [1800/4200], Loss: 0.0032\n",
      "Epoch [5/10], Step [1900/4200], Loss: 0.0069\n",
      "Epoch [5/10], Step [2000/4200], Loss: 0.0012\n",
      "Epoch [5/10], Step [2100/4200], Loss: 0.0015\n",
      "Epoch [5/10], Step [2200/4200], Loss: 0.0040\n",
      "Epoch [5/10], Step [2300/4200], Loss: 0.0006\n",
      "Epoch [5/10], Step [2400/4200], Loss: 0.0001\n",
      "Epoch [5/10], Step [2500/4200], Loss: 0.0002\n",
      "Epoch [5/10], Step [2600/4200], Loss: 0.0005\n",
      "Epoch [5/10], Step [2700/4200], Loss: 0.1464\n",
      "Epoch [5/10], Step [2800/4200], Loss: 0.0013\n",
      "Epoch [5/10], Step [2900/4200], Loss: 0.0372\n",
      "Epoch [5/10], Step [3000/4200], Loss: 0.0092\n",
      "Epoch [5/10], Step [3100/4200], Loss: 0.0004\n",
      "Epoch [5/10], Step [3200/4200], Loss: 0.0168\n",
      "Epoch [5/10], Step [3300/4200], Loss: 0.0058\n",
      "Epoch [5/10], Step [3400/4200], Loss: 0.0016\n",
      "Epoch [5/10], Step [3500/4200], Loss: 0.2679\n",
      "Epoch [5/10], Step [3600/4200], Loss: 0.0307\n",
      "Epoch [5/10], Step [3700/4200], Loss: 0.0083\n",
      "Epoch [5/10], Step [3800/4200], Loss: 0.4264\n",
      "Epoch [5/10], Step [3900/4200], Loss: 0.0016\n",
      "Epoch [5/10], Step [4000/4200], Loss: 0.0051\n",
      "Epoch [5/10], Step [4100/4200], Loss: 0.0035\n",
      "Epoch [5/10], Step [4200/4200], Loss: 0.0002\n",
      "Epoch [6/10], Step [100/4200], Loss: 0.0167\n",
      "Epoch [6/10], Step [200/4200], Loss: 0.0205\n",
      "Epoch [6/10], Step [300/4200], Loss: 0.0008\n",
      "Epoch [6/10], Step [400/4200], Loss: 0.0001\n",
      "Epoch [6/10], Step [500/4200], Loss: 0.0693\n",
      "Epoch [6/10], Step [600/4200], Loss: 0.0148\n",
      "Epoch [6/10], Step [700/4200], Loss: 0.0897\n",
      "Epoch [6/10], Step [800/4200], Loss: 0.0004\n",
      "Epoch [6/10], Step [900/4200], Loss: 0.0114\n",
      "Epoch [6/10], Step [1000/4200], Loss: 0.0362\n",
      "Epoch [6/10], Step [1100/4200], Loss: 0.0011\n",
      "Epoch [6/10], Step [1200/4200], Loss: 0.0030\n",
      "Epoch [6/10], Step [1300/4200], Loss: 0.0008\n",
      "Epoch [6/10], Step [1400/4200], Loss: 0.0682\n",
      "Epoch [6/10], Step [1500/4200], Loss: 0.0588\n",
      "Epoch [6/10], Step [1600/4200], Loss: 0.0076\n",
      "Epoch [6/10], Step [1700/4200], Loss: 0.0010\n",
      "Epoch [6/10], Step [1800/4200], Loss: 0.0026\n",
      "Epoch [6/10], Step [1900/4200], Loss: 0.0041\n",
      "Epoch [6/10], Step [2000/4200], Loss: 0.0223\n",
      "Epoch [6/10], Step [2100/4200], Loss: 0.0005\n",
      "Epoch [6/10], Step [2200/4200], Loss: 0.0049\n",
      "Epoch [6/10], Step [2300/4200], Loss: 0.1074\n",
      "Epoch [6/10], Step [2400/4200], Loss: 0.0021\n",
      "Epoch [6/10], Step [2500/4200], Loss: 0.0013\n",
      "Epoch [6/10], Step [2600/4200], Loss: 0.0094\n",
      "Epoch [6/10], Step [2700/4200], Loss: 0.0006\n",
      "Epoch [6/10], Step [2800/4200], Loss: 0.0134\n",
      "Epoch [6/10], Step [2900/4200], Loss: 0.1154\n",
      "Epoch [6/10], Step [3000/4200], Loss: 0.0015\n",
      "Epoch [6/10], Step [3100/4200], Loss: 0.0003\n",
      "Epoch [6/10], Step [3200/4200], Loss: 0.0565\n",
      "Epoch [6/10], Step [3300/4200], Loss: 0.0011\n",
      "Epoch [6/10], Step [3400/4200], Loss: 0.0000\n",
      "Epoch [6/10], Step [3500/4200], Loss: 0.0003\n",
      "Epoch [6/10], Step [3600/4200], Loss: 0.0007\n",
      "Epoch [6/10], Step [3700/4200], Loss: 0.0105\n",
      "Epoch [6/10], Step [3800/4200], Loss: 0.0116\n",
      "Epoch [6/10], Step [3900/4200], Loss: 0.0004\n",
      "Epoch [6/10], Step [4000/4200], Loss: 0.0196\n",
      "Epoch [6/10], Step [4100/4200], Loss: 0.0056\n",
      "Epoch [6/10], Step [4200/4200], Loss: 0.0009\n",
      "Epoch [7/10], Step [100/4200], Loss: 0.0003\n",
      "Epoch [7/10], Step [200/4200], Loss: 0.0018\n",
      "Epoch [7/10], Step [300/4200], Loss: 0.0295\n",
      "Epoch [7/10], Step [400/4200], Loss: 0.0003\n",
      "Epoch [7/10], Step [500/4200], Loss: 0.0004\n",
      "Epoch [7/10], Step [600/4200], Loss: 0.0898\n",
      "Epoch [7/10], Step [700/4200], Loss: 0.0002\n",
      "Epoch [7/10], Step [800/4200], Loss: 0.0013\n",
      "Epoch [7/10], Step [900/4200], Loss: 0.0001\n",
      "Epoch [7/10], Step [1000/4200], Loss: 0.0578\n",
      "Epoch [7/10], Step [1100/4200], Loss: 0.0001\n",
      "Epoch [7/10], Step [1200/4200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1300/4200], Loss: 0.1665\n",
      "Epoch [7/10], Step [1400/4200], Loss: 0.0204\n",
      "Epoch [7/10], Step [1500/4200], Loss: 0.0050\n",
      "Epoch [7/10], Step [1600/4200], Loss: 0.0153\n",
      "Epoch [7/10], Step [1700/4200], Loss: 0.0005\n",
      "Epoch [7/10], Step [1800/4200], Loss: 0.0000\n",
      "Epoch [7/10], Step [1900/4200], Loss: 0.0003\n",
      "Epoch [7/10], Step [2000/4200], Loss: 0.0121\n",
      "Epoch [7/10], Step [2100/4200], Loss: 0.0007\n",
      "Epoch [7/10], Step [2200/4200], Loss: 0.0035\n",
      "Epoch [7/10], Step [2300/4200], Loss: 0.1142\n",
      "Epoch [7/10], Step [2400/4200], Loss: 0.0025\n",
      "Epoch [7/10], Step [2500/4200], Loss: 0.0001\n",
      "Epoch [7/10], Step [2600/4200], Loss: 0.0013\n",
      "Epoch [7/10], Step [2700/4200], Loss: 0.0003\n",
      "Epoch [7/10], Step [2800/4200], Loss: 0.0009\n",
      "Epoch [7/10], Step [2900/4200], Loss: 0.0011\n",
      "Epoch [7/10], Step [3000/4200], Loss: 0.0296\n",
      "Epoch [7/10], Step [3100/4200], Loss: 0.0013\n",
      "Epoch [7/10], Step [3200/4200], Loss: 0.0850\n",
      "Epoch [7/10], Step [3300/4200], Loss: 0.1890\n",
      "Epoch [7/10], Step [3400/4200], Loss: 0.0276\n",
      "Epoch [7/10], Step [3500/4200], Loss: 0.0006\n",
      "Epoch [7/10], Step [3600/4200], Loss: 0.0034\n",
      "Epoch [7/10], Step [3700/4200], Loss: 0.0345\n",
      "Epoch [7/10], Step [3800/4200], Loss: 0.0000\n",
      "Epoch [7/10], Step [3900/4200], Loss: 0.0026\n",
      "Epoch [7/10], Step [4000/4200], Loss: 0.0531\n",
      "Epoch [7/10], Step [4100/4200], Loss: 0.5315\n",
      "Epoch [7/10], Step [4200/4200], Loss: 0.2585\n",
      "Epoch [8/10], Step [100/4200], Loss: 0.0005\n",
      "Epoch [8/10], Step [200/4200], Loss: 0.0041\n",
      "Epoch [8/10], Step [300/4200], Loss: 0.0056\n",
      "Epoch [8/10], Step [400/4200], Loss: 0.0020\n",
      "Epoch [8/10], Step [500/4200], Loss: 0.0108\n",
      "Epoch [8/10], Step [600/4200], Loss: 0.0163\n",
      "Epoch [8/10], Step [700/4200], Loss: 0.0002\n",
      "Epoch [8/10], Step [800/4200], Loss: 0.0088\n",
      "Epoch [8/10], Step [900/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [1000/4200], Loss: 0.0011\n",
      "Epoch [8/10], Step [1100/4200], Loss: 0.0017\n",
      "Epoch [8/10], Step [1200/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [1300/4200], Loss: 0.2683\n",
      "Epoch [8/10], Step [1400/4200], Loss: 0.0017\n",
      "Epoch [8/10], Step [1500/4200], Loss: 0.0025\n",
      "Epoch [8/10], Step [1600/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [1700/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [1800/4200], Loss: 0.0170\n",
      "Epoch [8/10], Step [1900/4200], Loss: 0.3689\n",
      "Epoch [8/10], Step [2000/4200], Loss: 0.1020\n",
      "Epoch [8/10], Step [2100/4200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2200/4200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2300/4200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2400/4200], Loss: 0.0789\n",
      "Epoch [8/10], Step [2500/4200], Loss: 0.0000\n",
      "Epoch [8/10], Step [2600/4200], Loss: 0.0045\n",
      "Epoch [8/10], Step [2700/4200], Loss: 0.0081\n",
      "Epoch [8/10], Step [2800/4200], Loss: 0.0290\n",
      "Epoch [8/10], Step [2900/4200], Loss: 0.1981\n",
      "Epoch [8/10], Step [3000/4200], Loss: 0.1536\n",
      "Epoch [8/10], Step [3100/4200], Loss: 0.0047\n",
      "Epoch [8/10], Step [3200/4200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3300/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [3400/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [3500/4200], Loss: 0.0010\n",
      "Epoch [8/10], Step [3600/4200], Loss: 0.0000\n",
      "Epoch [8/10], Step [3700/4200], Loss: 0.0011\n",
      "Epoch [8/10], Step [3800/4200], Loss: 0.0067\n",
      "Epoch [8/10], Step [3900/4200], Loss: 0.0444\n",
      "Epoch [8/10], Step [4000/4200], Loss: 0.0001\n",
      "Epoch [8/10], Step [4100/4200], Loss: 0.0627\n",
      "Epoch [8/10], Step [4200/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [100/4200], Loss: 0.0002\n",
      "Epoch [9/10], Step [200/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [300/4200], Loss: 0.0071\n",
      "Epoch [9/10], Step [400/4200], Loss: 0.0070\n",
      "Epoch [9/10], Step [500/4200], Loss: 0.0097\n",
      "Epoch [9/10], Step [600/4200], Loss: 0.1187\n",
      "Epoch [9/10], Step [700/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [800/4200], Loss: 0.0008\n",
      "Epoch [9/10], Step [900/4200], Loss: 0.0110\n",
      "Epoch [9/10], Step [1000/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [1100/4200], Loss: 0.0015\n",
      "Epoch [9/10], Step [1200/4200], Loss: 0.0327\n",
      "Epoch [9/10], Step [1300/4200], Loss: 0.0002\n",
      "Epoch [9/10], Step [1400/4200], Loss: 0.3128\n",
      "Epoch [9/10], Step [1500/4200], Loss: 0.0110\n",
      "Epoch [9/10], Step [1600/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [1700/4200], Loss: 0.0561\n",
      "Epoch [9/10], Step [1800/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [1900/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2000/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2100/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [2200/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2300/4200], Loss: 0.0010\n",
      "Epoch [9/10], Step [2400/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2500/4200], Loss: 0.0299\n",
      "Epoch [9/10], Step [2600/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [2700/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [2800/4200], Loss: 0.0245\n",
      "Epoch [9/10], Step [2900/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3000/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3100/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3200/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3300/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [3400/4200], Loss: 0.0032\n",
      "Epoch [9/10], Step [3500/4200], Loss: 0.0010\n",
      "Epoch [9/10], Step [3600/4200], Loss: 0.0017\n",
      "Epoch [9/10], Step [3700/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3800/4200], Loss: 0.0000\n",
      "Epoch [9/10], Step [3900/4200], Loss: 0.0004\n",
      "Epoch [9/10], Step [4000/4200], Loss: 0.0001\n",
      "Epoch [9/10], Step [4100/4200], Loss: 0.0119\n",
      "Epoch [9/10], Step [4200/4200], Loss: 0.0003\n",
      "Epoch [10/10], Step [100/4200], Loss: 0.0002\n",
      "Epoch [10/10], Step [200/4200], Loss: 0.0001\n",
      "Epoch [10/10], Step [300/4200], Loss: 0.0010\n",
      "Epoch [10/10], Step [400/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [500/4200], Loss: 0.0003\n",
      "Epoch [10/10], Step [600/4200], Loss: 0.0007\n",
      "Epoch [10/10], Step [700/4200], Loss: 0.0001\n",
      "Epoch [10/10], Step [800/4200], Loss: 0.0002\n",
      "Epoch [10/10], Step [900/4200], Loss: 0.0111\n",
      "Epoch [10/10], Step [1000/4200], Loss: 0.0066\n",
      "Epoch [10/10], Step [1100/4200], Loss: 0.1180\n",
      "Epoch [10/10], Step [1200/4200], Loss: 0.0001\n",
      "Epoch [10/10], Step [1300/4200], Loss: 0.0011\n",
      "Epoch [10/10], Step [1400/4200], Loss: 0.0018\n",
      "Epoch [10/10], Step [1500/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [1600/4200], Loss: 0.0118\n",
      "Epoch [10/10], Step [1700/4200], Loss: 0.0014\n",
      "Epoch [10/10], Step [1800/4200], Loss: 0.0055\n",
      "Epoch [10/10], Step [1900/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2000/4200], Loss: 0.0108\n",
      "Epoch [10/10], Step [2100/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2200/4200], Loss: 0.0001\n",
      "Epoch [10/10], Step [2300/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2400/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2500/4200], Loss: 0.0077\n",
      "Epoch [10/10], Step [2600/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [2700/4200], Loss: 0.0070\n",
      "Epoch [10/10], Step [2800/4200], Loss: 0.0072\n",
      "Epoch [10/10], Step [2900/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3000/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3100/4200], Loss: 0.0014\n",
      "Epoch [10/10], Step [3200/4200], Loss: 0.0069\n",
      "Epoch [10/10], Step [3300/4200], Loss: 0.0159\n",
      "Epoch [10/10], Step [3400/4200], Loss: 0.0013\n",
      "Epoch [10/10], Step [3500/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3600/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3700/4200], Loss: 0.0002\n",
      "Epoch [10/10], Step [3800/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [3900/4200], Loss: 0.0001\n",
      "Epoch [10/10], Step [4000/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4100/4200], Loss: 0.0000\n",
      "Epoch [10/10], Step [4200/4200], Loss: 0.0008\n",
      "Accuracy of the network on the test images: 97.21428571428571%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "torch.seed = 42\n",
    "# Define hyperparameters\n",
    "learning_rate = 5e-4\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += Y_test.size(0)\n",
    "    correct += (predicted == Y_test).sum().item()\n",
    "    print(f'Accuracy of the network on the test images: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = test_data[columns].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"res.csv\", \"w\") as f :\n",
    "    f.write(\"ImageId,Label\\n\")\n",
    "    \n",
    "for idx, img in enumerate(test_input) :\n",
    "    img_tensor = torch.tensor(img)\n",
    "    img_tensor = torch.reshape(img_tensor, (1, -1)) / 255\n",
    "    output = model(img_tensor)\n",
    "    _, digit = output.max(dim = 1)\n",
    "    sample_img = img\n",
    "\n",
    "    with open(\"res.csv\", \"a\") as f :\n",
    "        row = f\"{idx+1},{digit.item()}\\n\"\n",
    "        f.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
